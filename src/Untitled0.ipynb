{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Cb1XXKT1qw7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e97dde3-3fbd-4eee-bc41-282c0f54630e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'student_resource 3', 'student_resource 3.zip', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the uploaded file\n",
        "zip_file = 'student_resource 3.zip'  # Replace this with the actual filename\n",
        "\n",
        "# Unzip the folder\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# List the contents to verify\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wB3ysl2YskEd"
      },
      "outputs": [],
      "source": [
        "entity_unit_map = {\n",
        "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
        "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
        "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
        "    'wattage': {'kilowatt', 'watt', 'horsepower'},  # Added 'horsepower'\n",
        "    'item_volume': {'centilitre', 'fluid ounce', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
        "}\n",
        "\n",
        "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y3OR2-BRssCL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import multiprocessing\n",
        "import time\n",
        "from time import time as timer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "import requests\n",
        "import urllib\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LXXb6m2Yszu6"
      },
      "outputs": [],
      "source": [
        "def common_mistake(unit):\n",
        "    if unit in allowed_units:\n",
        "        return unit\n",
        "    if unit.replace('ter', 'tre') in allowed_units:\n",
        "        return unit.replace('ter', 'tre')\n",
        "    if unit.replace('feet', 'foot') in allowed_units:\n",
        "        return unit.replace('feet', 'foot')\n",
        "    return unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IT8AQLwntDu-"
      },
      "outputs": [],
      "source": [
        "def parse_string(s):\n",
        "    s_stripped = \"\" if s==None or str(s)=='nan' else s.strip()\n",
        "    if s_stripped == \"\":\n",
        "        return None, None\n",
        "    pattern = re.compile(r'^-?\\d+(\\.\\d+)?\\s+[a-zA-Z\\s]+$')\n",
        "    if not pattern.match(s_stripped):\n",
        "        raise ValueError(\"Invalid format in {}\".format(s))\n",
        "    parts = s_stripped.split(maxsplit=1)\n",
        "    number = float(parts[0])\n",
        "    unit = common_mistake(parts[1])\n",
        "    if unit not in allowed_units:\n",
        "        raise ValueError(\"Invalid unit [{}] found in {}. Allowed units: {}\".format(\n",
        "            unit, s, allowed_units))\n",
        "    return number, unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5t2rps6ytJc1"
      },
      "outputs": [],
      "source": [
        "def create_placeholder_image(image_save_path):\n",
        "    try:\n",
        "        placeholder_image = Image.new('RGB', (100, 100), color='black')\n",
        "        placeholder_image.save(image_save_path)\n",
        "    except Exception as e:\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5W93KfAGtQHB"
      },
      "outputs": [],
      "source": [
        "def download_image(image_link, save_folder, retries=3, delay=3):\n",
        "    if not isinstance(image_link, str):\n",
        "        return\n",
        "\n",
        "    filename = Path(image_link).name\n",
        "    image_save_path = os.path.join(save_folder, filename)\n",
        "\n",
        "    if os.path.exists(image_save_path):\n",
        "        return\n",
        "\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            response = requests.get(image_link, stream=True)\n",
        "            if response.status_code == 200:\n",
        "                with open(image_save_path, 'wb') as f:\n",
        "                    for chunk in response.iter_content(1024):\n",
        "                        f.write(chunk)\n",
        "                return\n",
        "            else:\n",
        "                print(f\"Failed to download {image_link}, status code: {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading image {image_link}: {e}\")\n",
        "        time.sleep(delay)\n",
        "\n",
        "    create_placeholder_image(image_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jfcghxOgrvVl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Constants\n",
        "DATASET_FOLDER = '/content/student_resource 3/dataset'\n",
        "IMAGE_FOLDER = '/content/student_resource 3/images'\n",
        "IMAGE_SIZE = (128, 128)  # Resize images\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "  # Limit the number of images\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zwws_z-5tWad"
      },
      "outputs": [],
      "source": [
        "def download_images(image_links, download_folder, allow_multiprocessing=True):\n",
        "    if not os.path.exists(download_folder):\n",
        "        os.makedirs(download_folder)\n",
        "\n",
        "    if allow_multiprocessing:\n",
        "        download_image_partial = partial(\n",
        "            download_image, save_folder=download_folder, retries=3, delay=3)\n",
        "\n",
        "        num_processes = min(multiprocessing.cpu_count(), 10)  # Limit to a reasonable number\n",
        "        with multiprocessing.Pool(num_processes) as pool:\n",
        "            list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))\n",
        "            pool.close()\n",
        "            pool.join()\n",
        "    else:\n",
        "        for image_link in tqdm(image_links, total=len(image_links)):\n",
        "            download_image(image_link, save_folder=download_folder, retries=3, delay=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LQKvqfKBuG-1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_entity_value(entity_value_str):\n",
        "    # Handle ranges (e.g., \"10 kilogram to 15 kilogram\")\n",
        "    range_match = re.match(r\"(\\d+(\\.\\d+)?)\\s*(\\w+)\\s*to\\s*(\\d+(\\.\\d+)?)\\s*(\\w+)\", entity_value_str)\n",
        "    if range_match:\n",
        "        value1 = float(range_match.group(1))\n",
        "        unit1 = range_match.group(3)\n",
        "        value2 = float(range_match.group(4))\n",
        "        unit2 = range_match.group(6)\n",
        "\n",
        "        if unit1 == unit2:\n",
        "            return (value1 + value2) / 2, unit1\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "    # Handle lists of values (e.g., \"[100.0, 240.0] volt\")\n",
        "    list_match = re.match(r\"\\[(\\d+(\\.\\d+)?),\\s*(\\d+(\\.\\d+)?)\\]\\s*(\\w+)\", entity_value_str)\n",
        "    if list_match:\n",
        "        value1 = float(list_match.group(1))\n",
        "        value2 = float(list_match.group(3))\n",
        "        unit = list_match.group(5)\n",
        "\n",
        "        return (value1 + value2) / 2, unit\n",
        "\n",
        "    # Handle standard format (e.g., \"10 kilogram\")\n",
        "    standard_match = re.match(r\"(\\d+(\\.\\d+)?)\\s*(\\w+)\", entity_value_str)\n",
        "    if standard_match:\n",
        "        value = float(standard_match.group(1))\n",
        "        unit = standard_match.group(3)\n",
        "        return value, unit\n",
        "\n",
        "    # Handle formats with possible plural forms (e.g., \"fluid ounces\", \"cubic inches\")\n",
        "    plural_match = re.match(r\"(\\d+(\\.\\d+)?)\\s*(\\w+)(s)?\", entity_value_str)\n",
        "    if plural_match:\n",
        "        value = float(plural_match.group(1))\n",
        "        unit = plural_match.group(3)\n",
        "        return value, unit\n",
        "\n",
        "    return None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aiIBJRU2rz30"
      },
      "outputs": [],
      "source": [
        "def normalize_unit(unit):\n",
        "    if unit is None:\n",
        "        return None\n",
        "\n",
        "    unit = unit.lower().strip()\n",
        "\n",
        "    # Unit mapping with common variations\n",
        "    unit_mapping = {\n",
        "        'fluid ounce': ['fluid ounce', 'fl oz', 'ounce fl', 'fl. oz.', 'fl.oz'],\n",
        "        'ounce': ['ounce', 'oz'],\n",
        "        'gallon': ['gallon', 'gal'],\n",
        "        'imperial gallon': ['imperial gallon', 'imp gal'],\n",
        "        'millilitre': ['millilitre', 'ml', 'milliliter'],\n",
        "        'microlitre': ['microlitre', 'µl', 'microliter'],\n",
        "        'litre': ['litre', 'l', 'liter'],\n",
        "        'centilitre': ['centilitre', 'cl'],\n",
        "        'decilitre': ['decilitre', 'dl'],\n",
        "        'cup': ['cup'],\n",
        "        'pint': ['pint'],\n",
        "        'quart': ['quart'],\n",
        "        'cubic inch': ['cubic inch', 'in³', 'cu in'],\n",
        "        'cubic foot': ['cubic foot', 'ft³', 'cu ft'],\n",
        "        'centimetre': ['centimetre', 'cm', 'centimeter'],\n",
        "        'foot': ['foot', 'ft'],\n",
        "        'inch': ['inch', 'in'],\n",
        "        'metre': ['metre', 'm', 'meter'],\n",
        "        'millimetre': ['millimetre', 'mm', 'millimeter'],\n",
        "        'yard': ['yard', 'yd'],\n",
        "        'gram': ['gram', 'g'],\n",
        "        'kilogram': ['kilogram', 'kg'],\n",
        "        'microgram': ['microgram', 'µg'],\n",
        "        'milligram': ['milligram', 'mg'],\n",
        "        'pound': ['pound', 'lb'],\n",
        "        'ton': ['ton', 't'],\n",
        "        'kilovolt': ['kilovolt', 'kv'],\n",
        "        'millivolt': ['millivolt', 'mv'],\n",
        "        'volt': ['volt', 'v'],\n",
        "        'kilowatt': ['kilowatt', 'kw'],\n",
        "        'watt': ['watt', 'w']\n",
        "    }\n",
        "\n",
        "    # Iterate through the unit_mapping dictionary to find the correct unit\n",
        "    for standard_unit, variations in unit_mapping.items():\n",
        "        if unit in variations:\n",
        "            return standard_unit\n",
        "\n",
        "    return unit  # Return original if no match is found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vjvEj9bUuknY"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "def process_chunk(chunk, image_folder, images, numerical_labels, unit_labels, entity_unit_map, IMAGE_SIZE):\n",
        "    for idx, row in chunk.iterrows():\n",
        "        img_name = os.path.basename(row['image_link']).split('.')[0] + '.jpg'\n",
        "        img_path = os.path.join(image_folder, img_name)\n",
        "\n",
        "        entity_value_str = row['entity_value']\n",
        "        entity_name = row['entity_name']\n",
        "\n",
        "        entity_value, unit = clean_entity_value(entity_value_str)\n",
        "\n",
        "        if entity_value is not None and unit is not None:\n",
        "            normalized_unit = normalize_unit(unit)  # Normalize the unit\n",
        "\n",
        "            # Validate entity_value and unit against allowed units for the specific entity\n",
        "            if normalized_unit in entity_unit_map.get(entity_name, set()):\n",
        "                if os.path.exists(img_path):  # Check if image exists before appending\n",
        "                    try:\n",
        "                        image = tf.keras.preprocessing.image.load_img(img_path, target_size=IMAGE_SIZE)\n",
        "                        image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "                        images.append(image)\n",
        "                        numerical_labels.append(float(entity_value))\n",
        "                        unit_labels.append(normalized_unit)\n",
        "                    except OSError as e:\n",
        "                        print(f\"Skipping image {img_path} due to error: {e}\")\n",
        "            else:\n",
        "                print(f\"Skipping row with unexpected entity_value format or unit: {entity_value_str}, entity: {entity_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping row due to invalid entity_value or missing unit: {entity_value_str}, entity: {entity_name}\")\n",
        "\n",
        "    # Clear unused variables to free memory\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "b2FATukqr3nz"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(csv_file, image_folder, max_images=200, max_entries=20000):\n",
        "    df = pd.read_csv(csv_file, nrows=max_entries)  # Limit rows read\n",
        "    df = df.sample(n=max_entries, random_state=52)  # Randomly select max_entries rows\n",
        "    download_images(df['image_link'].tolist(), image_folder)\n",
        "\n",
        "    images = []\n",
        "    numerical_labels = []\n",
        "    unit_labels = []\n",
        "\n",
        "    total_rows = len(df)\n",
        "    processed_rows = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        img_name = os.path.basename(row['image_link']).split('.')[0] + '.jpg'\n",
        "        img_path = os.path.join(image_folder, img_name)\n",
        "\n",
        "        entity_value_str = row['entity_value']\n",
        "        entity_name = row['entity_name']\n",
        "\n",
        "        # 2. Cleaning and handling entity value and unit\n",
        "        entity_value, unit = clean_entity_value(entity_value_str)\n",
        "\n",
        "        # 3. Skip invalid entries (keeping this functionality intact)\n",
        "        if entity_value is not None and unit is not None:\n",
        "            normalized_unit = normalize_unit(unit)\n",
        "\n",
        "            if normalized_unit in entity_unit_map.get(entity_name, set()):\n",
        "                if os.path.exists(img_path):  # Check if image exists before processing\n",
        "                    try:\n",
        "                        image = tf.keras.preprocessing.image.load_img(img_path, target_size=IMAGE_SIZE)\n",
        "                        image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "                        images.append(image)\n",
        "                        numerical_labels.append(float(entity_value))\n",
        "                        unit_labels.append(normalized_unit)\n",
        "                    except OSError as e:\n",
        "                        print(f\"Skipping image {img_path} due to error: {e}\")\n",
        "            else:\n",
        "                print(f\"Skipping row with unexpected entity_value format or unit: {entity_value_str}, entity: {entity_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping row due to invalid entity_value or missing unit: {entity_value_str}, entity: {entity_name}\")\n",
        "\n",
        "        processed_rows += 1\n",
        "        if processed_rows % 1000 == 0:  # Progress logging\n",
        "            print(f\"Processed {processed_rows}/{total_rows} rows\")\n",
        "\n",
        "    # Normalize images to range [0, 1]\n",
        "    images = np.array(images) / 255.0\n",
        "    numerical_labels = np.array(numerical_labels)\n",
        "    unit_labels = np.array(unit_labels)\n",
        "\n",
        "    # Encode units using LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    unit_labels = le.fit_transform(unit_labels)\n",
        "\n",
        "    # Output data shape for debugging\n",
        "    print(f'Images shape: {images.shape}')\n",
        "    print(f'Numerical labels shape: {numerical_labels.shape}')\n",
        "    print(f'Unit labels shape: {unit_labels.shape}')\n",
        "    print(f'Number of unique units: {len(le.classes_)}')\n",
        "\n",
        "    return images, numerical_labels, unit_labels, le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1ytujTTNsBhn"
      },
      "outputs": [],
      "source": [
        "def build_value_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)  # Adjust learning rate\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "g3Opu7rlsEEg"
      },
      "outputs": [],
      "source": [
        "def build_unit_model(num_classes):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),  # Added Dropout\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')  # Output for unit classification\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Adjusted learning rate\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9VqdruzkJhXa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def train_model():\n",
        "    # Load and preprocess data with a limit of 10,000 entries\n",
        "    train_images, train_numerical_labels, train_unit_labels, unit_le = load_and_preprocess_data(\n",
        "        os.path.join(DATASET_FOLDER, 'train.csv'), IMAGE_FOLDER, max_entries=10000\n",
        "    )\n",
        "\n",
        "    # Normalize image data to [0, 1] (for both models, if applicable)\n",
        "    train_images = train_images / 255.0\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_val, y_train_num, y_val_num, y_train_unit, y_val_unit = train_test_split(\n",
        "        train_images, train_numerical_labels, train_unit_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Scale numerical labels for the value model\n",
        "    scaler_num = StandardScaler()\n",
        "    y_train_num_scaled = scaler_num.fit_transform(y_train_num.reshape(-1, 1)).ravel()\n",
        "    y_val_num_scaled = scaler_num.transform(y_val_num.reshape(-1, 1)).ravel()\n",
        "\n",
        "    # Build models\n",
        "    value_model = build_value_model()\n",
        "    unit_model = build_unit_model(num_classes=len(unit_le.classes_))\n",
        "\n",
        "    # Define callbacks\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    model_checkpoint_value = tf.keras.callbacks.ModelCheckpoint(\n",
        "        'value_model.keras',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss'\n",
        "    )\n",
        "    model_checkpoint_unit = tf.keras.callbacks.ModelCheckpoint(\n",
        "        'unit_model.keras',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss'\n",
        "    )\n",
        "\n",
        "    # Train numerical value model\n",
        "    history_value = value_model.fit(\n",
        "        X_train, y_train_num_scaled,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=(X_val, y_val_num_scaled),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping, model_checkpoint_value]\n",
        "    )\n",
        "\n",
        "    # Train unit classification model\n",
        "    history_unit = unit_model.fit(\n",
        "        X_train, y_train_unit,\n",
        "        epochs=50,\n",
        "        validation_data=(X_val, y_val_unit),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping, model_checkpoint_unit]\n",
        "    )\n",
        "\n",
        "    print(\"Models trained and saved as 'value_model.keras' and 'unit_model.keras'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "YFDU2Dhnueoa",
        "outputId": "d2ab332b-35c7-4b0d-edfa-f745989860b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " 16%|█▋        | 1628/10000 [00:33<03:02, 45.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download https://m.media-amazon.com/images/I/1yw53vfQtS.jpg, status code: 400\n",
            "Failed to download https://m.media-amazon.com/images/I/1yw53vfQtS.jpg, status code: 400\n",
            "Failed to download https://m.media-amazon.com/images/I/1yw53vfQtS.jpg, status code: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 2446/10000 [00:57<02:56, 42.70it/s]Process ForkPoolWorker-2:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1c3c8b5f4c72>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-e0ad3f578f50>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load and preprocess data with a limit of 10,000 entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     train_images, train_numerical_labels, train_unit_labels, unit_le = load_and_preprocess_data(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_entries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-26-7dba3be7f6e9>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(csv_file, image_folder, max_images, max_entries)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_entries\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Limit rows read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_entries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Randomly select max_entries rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdownload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-28fe30cd2b49>\u001b[0m in \u001b[0;36mdownload_images\u001b[0;34m(image_links, download_folder, allow_multiprocessing)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnum_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Limit to a reasonable number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_image_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0UNQsSywwFVD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CkbOFQKKwDkN"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "DATASET_FOLDER = '/content/student_resource 3/dataset'\n",
        "IMAGE_FOLDER = '/content/student_resource 3/test_images'\n",
        "MODEL_VALUE_PATH = 'value_model.keras'\n",
        "MODEL_UNIT_PATH = 'unit_model.keras'\n",
        "IMAGE_SIZE = (128, 128)  # Same as defined during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-LNjDrSgwMTY"
      },
      "outputs": [],
      "source": [
        "def remove_directory(directory_path):\n",
        "    \"\"\"Remove directory and its contents.\"\"\"\n",
        "    if os.path.exists(directory_path):\n",
        "        shutil.rmtree(directory_path)\n",
        "    else:\n",
        "        print(f\"Directory {directory_path} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "fcDTVgMFwOdk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_models():\n",
        "    \"\"\"Load the trained models.\"\"\"\n",
        "    value_model = tf.keras.models.load_model(MODEL_VALUE_PATH)\n",
        "    unit_model = tf.keras.models.load_model(MODEL_UNIT_PATH)\n",
        "    return value_model, unit_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gyjVA8h6wQSo"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    \"\"\"Preprocess the image for model prediction.\"\"\"\n",
        "    img = tf.keras.utils.load_img(image_path, target_size=IMAGE_SIZE)\n",
        "    img_array = tf.keras.utils.img_to_array(img) / 255.0  # Normalize\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MfKwYvYRwUuP"
      },
      "outputs": [],
      "source": [
        "import absl.logging\n",
        "\n",
        "# Suppress TensorFlow warnings and absl logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "def predict_entity_value(image_path, value_model, unit_model, entity_name):\n",
        "    \"\"\"Predict entity value and unit from the image.\"\"\"\n",
        "    img_array = preprocess_image(image_path)\n",
        "\n",
        "    # Predict numerical value without showing the progress bar\n",
        "    numerical_prediction = value_model.predict(img_array, verbose=0)[0][0]\n",
        "\n",
        "    # Predict unit without showing the progress bar\n",
        "    unit_prediction = unit_model.predict(img_array, verbose=0)\n",
        "    unit_label = np.argmax(unit_prediction[0])\n",
        "\n",
        "    # Get list of units for the specific entity\n",
        "    units = entity_unit_map.get(entity_name, set())\n",
        "\n",
        "    # Ensure unit prediction is valid\n",
        "    unit = list(units)[unit_label] if unit_label < len(units) else None\n",
        "\n",
        "    if unit not in allowed_units:\n",
        "        unit = None\n",
        "\n",
        "    if unit is not None:\n",
        "        return f\"{numerical_prediction:.2f} {unit}\"\n",
        "    else:\n",
        "        return \"\"  # If unit is invalid or not found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "9GxxiLinwV1v"
      },
      "outputs": [],
      "source": [
        "def run_sanity_check(test_filename, output_filename):\n",
        "    \"\"\"Run sanity check using the sanity.py script.\"\"\"\n",
        "    try:\n",
        "        # Construct the command\n",
        "        command = [\n",
        "            \"python\", \"src/sanity.py\",\n",
        "            f\"--test_filename={test_filename}\",\n",
        "            f\"--output_filename={output_filename}\"\n",
        "        ]\n",
        "        # Run the command\n",
        "        subprocess.run(command, check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Sanity check failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SndPAigJwY5K"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Read the sample test dataset\n",
        "    sample_test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
        "\n",
        "    # Load models\n",
        "    value_model, unit_model = load_models()\n",
        "\n",
        "    # Download images\n",
        "    download_images(sample_test['image_link'], IMAGE_FOLDER)\n",
        "\n",
        "    # Ensure images were downloaded\n",
        "    if len(os.listdir(IMAGE_FOLDER)) == 0:\n",
        "        print(f\"No images found in {IMAGE_FOLDER}.\")\n",
        "        return\n",
        "\n",
        "    # Generate predictions with progress output\n",
        "    total_images = len(sample_test)\n",
        "    print(f\"Processing {total_images} images...\")\n",
        "\n",
        "    output_filename = os.path.join(DATASET_FOLDER, 'test_out.csv')\n",
        "\n",
        "    for idx, row in sample_test.iterrows():\n",
        "        image_path = os.path.join(IMAGE_FOLDER, os.path.basename(row['image_link']))\n",
        "        prediction = predict_entity_value(image_path, value_model, unit_model, row['entity_name'])\n",
        "        sample_test.at[idx, 'prediction'] = prediction\n",
        "\n",
        "        # Save the prediction row-by-row to avoid holding all data in memory\n",
        "        sample_test[['index', 'prediction']].iloc[[idx]].to_csv(\n",
        "            output_filename,\n",
        "            mode='a',\n",
        "            header=not os.path.exists(output_filename),\n",
        "            index=False\n",
        "        )\n",
        "\n",
        "        # Print progress every 1000 images or at the end\n",
        "        if (idx + 1) % 1000 == 0 or idx + 1 == total_images:\n",
        "            print(f\"Processed {idx + 1}/{total_images} images\")\n",
        "\n",
        "    # Run sanity check after all images are processed\n",
        "    run_sanity_check(os.path.join(DATASET_FOLDER, 'sample_test.csv'), output_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ619rt9weaG",
        "outputId": "4998ddff-9bf6-4620-a4c0-b318a14c6333"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " 40%|████      | 52993/131187 [10:59<17:13, 75.62it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error downloading image https://m.media-amazon.com/images/I/51ZIuXoFxrL.jpg: HTTPSConnectionPool(host='m.media-amazon.com', port=443): Max retries exceeded with url: /images/I/51ZIuXoFxrL.jpg (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007)')))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 131187/131187 [32:13<00:00, 67.85it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 131187 images...\n",
            "Processed 1000/131187 images\n",
            "Processed 2000/131187 images\n",
            "Processed 3000/131187 images\n",
            "Processed 4000/131187 images\n",
            "Processed 5000/131187 images\n",
            "Processed 6000/131187 images\n",
            "Processed 7000/131187 images\n",
            "Processed 8000/131187 images\n",
            "Processed 9000/131187 images\n",
            "Processed 10000/131187 images\n",
            "Processed 11000/131187 images\n",
            "Processed 12000/131187 images\n",
            "Processed 13000/131187 images\n",
            "Processed 14000/131187 images\n",
            "Processed 15000/131187 images\n",
            "Processed 16000/131187 images\n",
            "Processed 17000/131187 images\n",
            "Processed 18000/131187 images\n",
            "Processed 19000/131187 images\n",
            "Processed 20000/131187 images\n",
            "Processed 21000/131187 images\n",
            "Processed 22000/131187 images\n",
            "Processed 23000/131187 images\n",
            "Processed 24000/131187 images\n",
            "Processed 25000/131187 images\n",
            "Processed 26000/131187 images\n",
            "Processed 27000/131187 images\n",
            "Processed 28000/131187 images\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rmdir /content/student_resource_3/models\n"
      ],
      "metadata": {
        "id": "9GFjf3Z92VBy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRSWd0xczT2W"
      },
      "outputs": [],
      "source": [
        "!python /content/student_resource\\ 3/src/sanity.py --test_filename \"/content/student_resource 3/dataset/sample_test_out.csv\" --output_filename \"/content/student_resource 3/dataset/test_out.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05hsG-OwfR_"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}